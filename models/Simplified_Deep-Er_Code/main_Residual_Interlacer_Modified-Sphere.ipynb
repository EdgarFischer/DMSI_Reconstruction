{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "In the Data folder file I showed that t>20 basically corresponds to noise. Here I want to check if the network performance improves if I throw out this noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= '2' #, this way I would choose GPU 3 to do the work\n",
    "\n",
    "sys.path.append('../../scripts')\n",
    "sys.path.append('../../models')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import zoom # for compressing images / only for testing purposes to speed up NN training\n",
    "from scipy.fft import fft2, fftshift\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from data_preparation import *\n",
    "from data_undersampling import *\n",
    "from interlacer_layer_modified import *\n",
    "from Residual_Interlacer_modified import *\n",
    "from output_statistics import *\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure as ssim\n",
    "from torchmetrics.image.ssim import StructuralSimilarityIndexMeasure\n",
    "from torchmetrics.image import PeakSignalNoiseRatio \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trancuate_t = 96 # set this parameter to control at which time step you stop using the signal\n",
    "\n",
    "grouped_time_steps = 1 # Set how many subsequent time steps you want to give to the network at once. Values allowed: 1, 2, 4, 8 (because it has to divide 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "1. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = np.load('../../data/combined_trancuated_k_space_low_rank_15.npy')\n",
    "combined_data = combined_data[:, :, :, :trancuate_t, :, :] # throw out t > 20 in this line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "2.Train / Test split;  Fourier transform and undersampling, normalization etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I make a very simple split - I leave the last subject as test_set (I use data of 5 subjects)\n",
    "undersampling_factor = 0.0001 #set undersampling fraction\n",
    "strategy = \"uniform_complementary\"\n",
    "fixed_radius = 6.9\n",
    "normalize = True\n",
    "combine = True\n",
    "\n",
    "#### Train_Test_Split ####\n",
    "training_images = combined_data[:,:,:,:,:,:4]  # Method: Leave last MRSI measurement as test set\n",
    "test_images = combined_data[:,:,:,:,:,4]\n",
    "\n",
    "#### group time steps, undersample in k-space, prepare NN Input, normalize if you want ####\n",
    "training_images, test_images, NN_input_train, NN_input_test, training_undersampled, test_undersampled, abs_test_set = preprocess_and_undersample(\n",
    "                                                                                                                        training_images,\n",
    "                                                                                                                        test_images,\n",
    "                                                                                                                        grouped_time_steps=grouped_time_steps, \n",
    "                                                                                                                        undersampling_factor=undersampling_factor,\n",
    "                                                                                                                        strategy = strategy,\n",
    "                                                                                                                        fixed_radius=fixed_radius,\n",
    "                                                                                                                        normalize = normalize\n",
    "                                                                                                                    )\n",
    "#### reshape for pytorch ####\n",
    "train_data = reshape_for_pytorch(NN_input_train,grouped_time_steps)\n",
    "train_labels = reshape_for_pytorch(training_images,grouped_time_steps)\n",
    "\n",
    "test_data = reshape_for_pytorch(NN_input_test,grouped_time_steps)\n",
    "test_labels = reshape_for_pytorch(test_images,grouped_time_steps)\n",
    "\n",
    "# Prepare k-space data (reshape undersampled k-space as well)\n",
    "train_k_space = reshape_for_pytorch(training_undersampled, grouped_time_steps)\n",
    "test_k_space = reshape_for_pytorch(test_undersampled, grouped_time_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_images.shape)\n",
    "\n",
    "Time_Step = 1\n",
    "\n",
    "slice_data = training_images[:,:,0,0]\n",
    "absolute_slice = np.abs(slice_data)\n",
    "\n",
    "plt.imshow(absolute_slice, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "4. Reshaping arrays to prepare for NN training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Load things up..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=50\n",
    "\n",
    "# Create TensorDataset instances with the correct arguments\n",
    "train_dataset = TensorDataset_interlacer(\n",
    "    k_space=train_k_space,  # Undersampled k-space input\n",
    "    image_reconstructed=train_data,  # Reconstructed image input\n",
    "    ground_truth=train_labels  # Fully sampled ground truth\n",
    ")\n",
    "\n",
    "test_dataset = TensorDataset_interlacer(\n",
    "    k_space=test_k_space,  # Undersampled k-space input\n",
    "    image_reconstructed=test_data,  # Reconstructed image input\n",
    "    ground_truth=test_labels  # Fully sampled ground truth\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Next I set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set the parameters for the Interlacer model\n",
    "features_img = 64           # Number of features in the image domain\n",
    "features_kspace = 64        # Number of features in the frequency domain\n",
    "kernel_size = 3             # Kernel size for the convolutional layers\n",
    "use_norm = \"BatchNorm\"      # Normalization type (\"BatchNorm\", \"InstanceNorm\", or \"None\")\n",
    "num_convs = 1              # Number of convolutional layers\n",
    "num_layers = 1              # Number of interlacer layers\n",
    "\n",
    "# Instantiate the Interlacer model\n",
    "model = ResidualInterlacerModified(\n",
    "    kernel_size=kernel_size,\n",
    "    num_features_img=features_img,\n",
    "    num_features_kspace=features_kspace,\n",
    "    num_convs=num_convs,\n",
    "    num_layers = num_layers,\n",
    "    use_norm=use_norm\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, loss_fn, data_loader, device='cpu'):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    num_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for dat_in, dat_out in data_loader:\n",
    "            dat_in, dat_out = dat_in.to(device), dat_out.to(device)  # Move to device\n",
    "            predictions, C2 = model(dat_in)  # Forward pass\n",
    "            loss_curr = loss_fn(predictions, dat_out, model, C2) # Compute loss\n",
    "            total_loss += loss_curr.item() * dat_in.size(0)  # Accumulate loss\n",
    "            num_samples += dat_in.size(0)  # Count samples\n",
    "    \n",
    "    # Return the average loss for the dataset\n",
    "    return total_loss / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainign loop and logging\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00002)\n",
    "loss_fn = CustomLoss() # note that the lambda parameter was defined in the automap paper, to additionally encourage spare representations.\n",
    "model = model.to(device)\n",
    "\n",
    "num_epochs = 500  # Number of epochs to train\n",
    "print_every = 5  # Print every 100 epochs\n",
    "\n",
    "psnr_metric = PeakSignalNoiseRatio(data_range=1.0).to(device)\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "writer = SummaryWriter(log_dir='runs/my_experiment')\n",
    "\n",
    "\n",
    "# Initialize lists to store loss and MSE values\n",
    "train_mses = []\n",
    "train_psnrs = []\n",
    "train_ssims = []\n",
    "\n",
    "test_mses = []\n",
    "test_psnrs = []\n",
    "test_ssims = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    _ = train_one_epoch(model, optimizer, loss_fn, train_loader, device=device)\n",
    "\n",
    "    psnr_metric.reset()\n",
    "    ssim_metric.reset()\n",
    "    \n",
    "    avg_loss_train, avg_psnr_train, avg_ssim_train = validate_model(\n",
    "            model, loss_fn, train_loader, device=device,\n",
    "            psnr_metric=psnr_metric,\n",
    "            ssim_metric=ssim_metric\n",
    "        )\n",
    "    \n",
    "    psnr_metric.reset()\n",
    "    ssim_metric.reset()\n",
    "    # We pass references to our metrics so we can compute them\n",
    "    avg_loss_test, avg_psnr_test, avg_ssim_test = validate_model(\n",
    "            model, loss_fn, test_loader, device=device,\n",
    "            psnr_metric=psnr_metric,\n",
    "            ssim_metric=ssim_metric\n",
    "        )\n",
    "    \n",
    "    train_mses.append(avg_loss_train)\n",
    "    train_psnrs.append(avg_psnr_train)\n",
    "    train_ssims.append(avg_ssim_train)\n",
    "    \n",
    "    test_mses.append(avg_loss_test)\n",
    "    test_psnrs.append(avg_psnr_test)\n",
    "    test_ssims.append(avg_ssim_test)\n",
    "    \n",
    "    writer.add_scalar('Loss/Train', avg_loss_train, epoch)\n",
    "    writer.add_scalar('Loss/Test', avg_loss_test, epoch)\n",
    "    writer.add_scalar('Metric/PSNR /Train', avg_psnr_train, epoch)\n",
    "    writer.add_scalar('Metric/PSNR /Test', avg_psnr_test, epoch)\n",
    "    writer.add_scalar('Metric/SSIM /Train', avg_ssim_train, epoch)\n",
    "    writer.add_scalar('Metric/SSIM/Test', avg_ssim_test, epoch)\n",
    "    \n",
    "    psnr_metric.reset()\n",
    "    ssim_metric.reset()\n",
    "    \n",
    "    # Print or log to console\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"   Train Loss: {avg_loss_train:.6f}\")\n",
    "        print(f\"   Test  Loss: {avg_loss_test:.6f}\")\n",
    "        print(f\"   Train  PSNR: {avg_psnr_train:.4f}\")\n",
    "        print(f\"   Test  PSNR: {avg_psnr_test:.4f}\")\n",
    "        print(f\"   Train  SSIM: {avg_ssim_train:.4f}\")\n",
    "        print(f\"   Test  SSIM: {avg_ssim_test:.4f}\\n\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot training and test losses\n",
    "plt.plot(range(1, num_epochs + 1), train_mses, label=\"Training Loss (MSE)\")\n",
    "plt.plot(range(1, num_epochs + 1), test_mses, label=\"Test Loss (MSE)\")\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.legend()\n",
    "\n",
    "# Show grid and display the plot\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Outputs_Test_Set, _ = Process_Model_Output_deeper(test_loader, model, device, trancuate_t, 8, grouped_time_steps, abs_test_set)\n",
    "\n",
    "combined_data = np.load('../../data/combined_data_low_rank_15.npy')\n",
    "Ground_Truth = combined_data[..., 4]\n",
    "\n",
    "plot_general_statistics(Model_Outputs_Test_Set, Ground_Truth, trancuate_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "t= 4\n",
    "T= 7\n",
    "\n",
    "Model_Outputs_Test_Set, _ = Process_Model_Output_deeper(test_loader, model, device, trancuate_t, 8, grouped_time_steps, abs_test_set)\n",
    "\n",
    "combined_data = np.load('../../data/combined_data_low_rank_15.npy')\n",
    "Ground_Truth = combined_data[..., 4]\n",
    "\n",
    "comparison_Plot_3D(Model_Outputs_Test_Set, Ground_Truth, t, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison in spectral domain \n",
    "tf= 50\n",
    "T= 7\n",
    "domain = \"spectral\"\n",
    "\n",
    "Model_Outputs_Test_Set, _ = Process_Model_Output_deeper(test_loader, model, device, trancuate_t, 8, grouped_time_steps, abs_test_set)\n",
    "combined_data = np.load('../../data/combined_data_low_rank_15.npy')\n",
    "ground_truth = combined_data[..., 4]\n",
    "\n",
    "comparison_Plot_3D(Model_Outputs_Test_Set, ground_truth, tf, T, domain = domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
