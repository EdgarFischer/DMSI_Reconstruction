{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa2be381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('../scripts')\n",
    "sys.path.append('../models')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= '0' #, this way I would choose GPU 3 to do the work\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import zoom # for compressing images / only for testing purposes to speed up NN training\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from data_preparation import *\n",
    "from data_undersampling import *\n",
    "from Naive_CNN_2D import *\n",
    "from output_statistics import *\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure as ssim\n",
    "from torchmetrics.image.ssim import StructuralSimilarityIndexMeasure\n",
    "from torchmetrics.image import PeakSignalNoiseRatio \n",
    "\n",
    "grouped_time_steps = 1 # Set how many subsequent time steps you want to give to the network at once. Values allowed: 1, 2, 4, 8 (because it has to divide 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01759b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "trancuate_t = 30\n",
    "#### Undersampling Strategy:#####\n",
    "Undersampling = \"Regular\" # Options: Regular or Possoin or Complementary_Masks\n",
    "Sampling_Mask = \"Complementary_Masks\" #Options: Single_Combination or One_Mask or Complementary_Masks\n",
    "AF = 2 #  acceleration factor\n",
    "\n",
    "#### Model Input and Output ####\n",
    "GT_Data = \"LowRank\" # Options: FullRank LowRank for GROUNDTRUTH!\n",
    "Low_Rank_Input = True ## apply low rank to the input as well if True\n",
    "\n",
    "####M Model Parameters ####\n",
    "batch_size=4096\n",
    "num_epochs = 500\n",
    "print_every = 1\n",
    "num_convs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c2c362",
   "metadata": {},
   "source": [
    "1. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9bb4c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define ground truth path####\n",
    "if GT_Data == \"FullRank\":\n",
    "    ground_truth_path = \"../data/Ground_Truth/Full_Rank/P03-P08_truncated_k_space.npy\"\n",
    "elif GT_Data == \"LowRank\":\n",
    "    ground_truth_path = \"../data/Ground_Truth/Low_Rank/LR_8_P03-P08_self.npy\"\n",
    "\n",
    "#### Assemble saving path ####\n",
    "\n",
    "#### Definie Model path\n",
    "if GT_Data == \"FullRank\":\n",
    "    model_save_dir = f\"../saved_models/Naive_CNN_2D/Full2Full/\"+Undersampling+f'/AF_{AF}/'+f'Truncate_t_{trancuate_t}/'+Sampling_Mask+f'/{num_convs}Layer'\n",
    "elif GT_Data == \"LowRank\":\n",
    "    model_save_dir = f\"../saved_models/Naive_CNN_2D/Low2Low/\"+Undersampling+f'/AF_{AF}/'+f'Truncate_t_{trancuate_t}/'+Sampling_Mask+f'/{num_convs}Layer'\n",
    "    \n",
    "#### Define log directory path\n",
    "if GT_Data == \"FullRank\":\n",
    "    log_dir = f\"../log_files/Naive_CNN_2D/Full2Full/\"+Undersampling+f'/AF_{AF}/'+f'Truncate_t_{trancuate_t}/'+Sampling_Mask+f'/{num_convs}Layer'\n",
    "elif GT_Data == \"LowRank\":\n",
    "    log_dir = f\"../log_files/Naive_CNN_2D/Low2Low/\"+Undersampling+f'/AF_{AF}/'+f'Truncate_t_{trancuate_t}/'+Sampling_Mask+f'/{num_convs}Layer'\n",
    "\n",
    "#### Define Input Data path\n",
    "undersampled_data_path = \"../data/Undersampled_Data/\"+Undersampling+f'/AF_{AF}/'+Sampling_Mask+'/data.npy'\n",
    "\n",
    "#### load data\n",
    "Ground_Truth = np.load(ground_truth_path)\n",
    "Undersampled_Data = np.load(undersampled_data_path)\n",
    "\n",
    "#### additionally make LowRank 8 transformation on input of network, this improves the error significantly!\n",
    "if Low_Rank_Input:\n",
    "    Undersampled_Data[...,0] = low_rank(Undersampled_Data[...,0], 8)\n",
    "    Undersampled_Data[...,1] = low_rank(Undersampled_Data[...,1], 8)\n",
    "    Undersampled_Data[...,2] = low_rank(Undersampled_Data[...,2], 8)\n",
    "    Undersampled_Data[...,3] = low_rank(Undersampled_Data[...,3], 8)\n",
    "    Undersampled_Data[...,4] = low_rank(Undersampled_Data[...,4], 8)\n",
    "    Undersampled_Data[...,5] = low_rank(Undersampled_Data[...,5], 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc113b12",
   "metadata": {},
   "source": [
    "2.Train / Test split;  Fourier transform and undersampling, reshaping etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1882235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Train_Test_Split ####\n",
    "ground_truth_train, ground_truth_test = Ground_Truth[:,:,:,:trancuate_t,:,:5], Ground_Truth[:,:,:,:trancuate_t,:,5]  # Method: Leave last MRSI measurement as test set\n",
    "\n",
    "#### Assign undersampled network input ####\n",
    "NN_input_train, NN_input_test = Undersampled_Data[:,:,:,:trancuate_t,:,:5], Undersampled_Data[:,:,:,:trancuate_t,:,5]\n",
    "\n",
    "# #### Reshape so (t,T) are first ####\n",
    "ground_truth_train, ground_truth_test = ground_truth_train.transpose(3, 4, 0, 1, 2, 5), ground_truth_test.transpose(3, 4, 0, 1, 2)\n",
    "NN_input_train, NN_input_test = NN_input_train.transpose(3, 4, 0, 1, 2, 5), NN_input_test.transpose(3, 4, 0, 1, 2)\n",
    "\n",
    "# #### Collapse ununsed dimensions ####\n",
    "ground_truth_train, ground_truth_test = ground_truth_train.reshape(trancuate_t, 8, -1), ground_truth_test.reshape(trancuate_t, 8, -1)\n",
    "NN_input_train, NN_input_test = NN_input_train.reshape(trancuate_t, 8, -1), NN_input_test.reshape(trancuate_t, 8, -1)\n",
    "\n",
    "# # #### reshape for pytorch ####\n",
    "train_data, train_labels  = reshape_for_pytorch_2D(NN_input_train, trancuate_t), reshape_for_pytorch_2D(ground_truth_train, trancuate_t)\n",
    "test_data, test_labels = reshape_for_pytorch_2D(NN_input_test, trancuate_t), reshape_for_pytorch_2D(ground_truth_test, trancuate_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da7c8fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50820, 2, 8, 30)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3e7317",
   "metadata": {},
   "source": [
    "Load things up..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dd0bf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDataset instances\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27f5688",
   "metadata": {},
   "source": [
    "Next I set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ff26c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive_CNN_2D(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  )\n",
      "  (final_conv): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Naive_CNN_2D(in_channels=2, num_convs=num_convs).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9777807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing model at ../saved_models/Naive_CNN_2D/Low2Low/Regular/AF_2/Truncate_t_30/Complementary_Masks/5Layer/latest_model.pth. Resuming training...\n",
      "New best model found at epoch 7 with Test MSE = 1342452.271670\n",
      "Saved to ../saved_models/Naive_CNN_2D/Low2Low/Regular/AF_2/Truncate_t_30/Complementary_Masks/5Layer/best_model.pth\n",
      "\n",
      "Epoch 7/500\n",
      "   Train Loss: 1360556.657615\n",
      "   Test  Loss: 1342452.271670\n",
      "\n",
      "New best model found at epoch 8 with Test MSE = 1319710.460326\n",
      "Saved to ../saved_models/Naive_CNN_2D/Low2Low/Regular/AF_2/Truncate_t_30/Complementary_Masks/5Layer/best_model.pth\n",
      "\n",
      "Epoch 8/500\n",
      "   Train Loss: 1335641.634780\n",
      "   Test  Loss: 1319710.460326\n",
      "\n",
      "New best model found at epoch 9 with Test MSE = 1292183.851855\n",
      "Saved to ../saved_models/Naive_CNN_2D/Low2Low/Regular/AF_2/Truncate_t_30/Complementary_Masks/5Layer/best_model.pth\n",
      "\n",
      "Epoch 9/500\n",
      "   Train Loss: 1305279.371606\n",
      "   Test  Loss: 1292183.851855\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6284/1163670341.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# ---- Evaluate on TRAIN data (returns average MSE) ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     avg_loss_train = validate_model(\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/hfish/Deuterium_Reconstruction/models/Naive_CNN_2D.py\u001b[0m in \u001b[0;36mvalidate_model\u001b[0;34m(model, loss_fn, data_loader, device)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdat_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdat_out\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0;31m# Move data to the specified device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mdat_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdat_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdat_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdat_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/hfish/Deuterium_Reconstruction/models/Naive_CNN_2D.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPyTorch\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \"\"\"\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Shape: (2, x, y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Shape: (2, x, y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "latest_model_path = os.path.join(model_save_dir, 'latest_model.pth')\n",
    "best_model_path   = os.path.join(model_save_dir, 'best_model.pth')\n",
    "\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "#  Initialize model, optimizer, loss, etc.\n",
    "# ------------------------------------------------------------------------\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00002)\n",
    "loss_fn = CustomLoss()  # Your custom loss\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Tracking lists for plotting/analysis\n",
    "train_mses = []\n",
    "test_mses = []\n",
    "\n",
    "# Track start epoch & best MSE so far\n",
    "start_epoch = 0\n",
    "best_test_mse = float('inf')  # We'll update this if we find a new best\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "#  If a \"latest model\" checkpoint exists, resume training from there\n",
    "# ------------------------------------------------------------------------\n",
    "if os.path.exists(latest_model_path):\n",
    "    print(f\"Found existing model at {latest_model_path}. Resuming training...\")\n",
    "    checkpoint = torch.load(latest_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    start_epoch   = checkpoint['epoch']\n",
    "    train_mses    = checkpoint.get('train_mses', [])\n",
    "    test_mses     = checkpoint.get('test_mses', [])\n",
    "    best_test_mse = checkpoint.get('best_test_mse', float('inf'))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "#  Main training loop\n",
    "# ------------------------------------------------------------------------\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    # ---- Train for one epoch ----\n",
    "    _ = train_one_epoch(model, optimizer, loss_fn, train_loader, device=device)\n",
    "\n",
    "    # ---- Evaluate on TRAIN data (returns average MSE) ----\n",
    "    avg_loss_train = validate_model(\n",
    "        model, loss_fn, train_loader, device=device\n",
    "    )\n",
    "\n",
    "    # ---- Evaluate on TEST data (returns average MSE) ----\n",
    "    avg_loss_test = validate_model(\n",
    "        model, loss_fn, test_loader, device=device\n",
    "    )\n",
    "\n",
    "    # ---- (Optional) Scheduler step ----\n",
    "    # scheduler.step(avg_loss_train)\n",
    "\n",
    "    # ---- Update tracked lists ----\n",
    "    train_mses.append(avg_loss_train)\n",
    "    test_mses.append(avg_loss_test)\n",
    "\n",
    "    # ---- TensorBoard Logging ----\n",
    "    writer.add_scalar('Loss/Train', avg_loss_train, epoch)\n",
    "    writer.add_scalar('Loss/Test',  avg_loss_test,  epoch)\n",
    "\n",
    "    # ---- Always save the \"latest\" model checkpoint ----\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_mses': train_mses,\n",
    "        'test_mses': test_mses,\n",
    "        'best_test_mse': best_test_mse\n",
    "    }, latest_model_path)\n",
    "\n",
    "    # ---- If we found a new \"best\" model on the TEST set, save separately ----\n",
    "    if avg_loss_test < best_test_mse:\n",
    "        best_test_mse = avg_loss_test\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_mses': train_mses,\n",
    "            'test_mses': test_mses,\n",
    "            'best_test_mse': best_test_mse\n",
    "        }, best_model_path)\n",
    "        print(f\"New best model found at epoch {epoch+1} with Test MSE = {avg_loss_test:.6f}\")\n",
    "        print(f\"Saved to {best_model_path}\\n\")\n",
    "\n",
    "    # ---- Print progress every 'print_every' epochs ----\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"   Train Loss: {avg_loss_train:.6f}\")\n",
    "        print(f\"   Test  Loss: {avg_loss_test:.6f}\\n\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
